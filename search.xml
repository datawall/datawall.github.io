<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>HBase之HFile索引机制二HFile索引结构解析</title>
      <link href="/2019/04/26/HBase%E4%B9%8BHFile%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E4%BA%8C/"/>
      <url>/2019/04/26/HBase%E4%B9%8BHFile%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#索引分层的原因">索引分层的原因</a></li><li><a href="#v2版本index-block有两类">V2版本Index Block有两类</a><ul><li><a href="#root-index-block">Root Index Block</a></li><li><a href="#nonroot-index-block">NonRoot Index Block</a></li></ul></li><li><a href="#hfile数据完整索引流程">HFile数据完整索引流程</a></li><li><a href="#索引块分裂">索引块分裂</a><ul><li><a href="#append流程">Append流程</a></li><li><a href="#finalize阶段">Finalize阶段</a></li></ul></li><li><a href="#总-结">总 结</a></li></ul><!-- tocstop --><h1><span id="索引分层的原因">索引分层的原因</span></h1><hr><p>HFile中索引结构根据索引层级的不同分为两种：single-level和mutil-level，前者表示单层索引，后者表示多级索引，一般为两级或三级。HFile V1版本中只有single-level一种索引结构，V2版本中引入多级索引。之所以引入多级索引，是因为随着HFile文件越来越大，Data Block越来越多，索引数据也越来越大，已经无法全部加载到内存中（V1版本中一个Region Server的索引数据加载到内存会占用几乎6G空间），多级索引可以只加载部分索引，降低内存使用空间。上一篇文章 《HBase-存储文件HFile结构解析》，我们提到Bloom    Filter内存使用问题是促使V1版本升级到V2版本的一个原因，再加上这个原因，这两个原因就是V1版本升级到V2版本最重要的两个因素。     </p><h1><span id="v2版本index-block有两类">V2版本Index Block有两类</span></h1><hr><p>分为Root Index Block和NonRoot Index Block，其中NonRoot Index Block又分为Intermediate Index Block和Leaf Index Block两种。HFile中索引结构类似于一棵树，Root Index Block表示索引数根节点，Intermediate Index Block表示中间节点，Leaf Index block表示叶子节点，叶子节点直接指向实际数据块。</p><p><img src="/images/pasted-36.png" alt="upload successful"></p><p>HFile中除了Data Block需要索引之外，上一篇文章提到过Bloom Block也需要索引，索引结构实际上就是采用了single-level结构，文中<strong>Bloom Index Block就是一种Root Index Block</strong>。</p><p>对于Data Block，<strong>由于HFile刚开始数据量较小，索引采用single-level结构，只有Root Index一层索引，直接指向数据块。当数据量慢慢变大，Root Index Block满了之后，索引就会变为mutil-level结构，由一层索引变为两层，根节点指向叶子节点，叶子节点指向实际数据块。如果数据量再变大，索引层级就会变为三层</strong>。</p><p>下面就针对Root index Block和NonRoot index Block两种结构进行解析，因为Root Index Block已经在上面一篇文章中分析过， 此处简单带过，重点介绍NonRoot Index Block结构（InterMediate Index Block和Ieaf Index Block在内存和磁盘中存储格式相同，都为NonRoot Index Block格式）。</p><h2><span id="root-index-block">Root Index Block</span></h2><p>Root Index Block表示索引树根节点索引块，可以作为bloom的直接索引，也可以作为data索引的根索引。而且对于single-level 和mutil-level两种索引结构对应的Root Index Block略有不同，<strong>本文以mutil-level索引结构为例进行分析（single-level索引结构是mutual-level的一种简化场景）</strong>，在内存和磁盘中的格式如下图所示：   </p><p><img src="/images/pasted-37.png" alt="upload successful"><br>其中Index Entry表示具体的索引对象，每个索引对象由3个字段组成    </p><ol><li>Block Offset表示索引指向数据块的偏移量</li><li>BlockDataSize 表示索引指向数据块在磁盘上的大小</li><li>BlockKey表示索引指向数据块中的第一个key</li><li>除此之外，还有另外3个字段用来记录MidKey的相关信息，<strong>MidKey表示HFile所有Data Block中中间的一个Data Block，用于在对HFile进行split操作时，快速定位HFile的中间位置</strong>。  </li></ol><p><strong>需要注意的是single-level索引结构和mutil-level结构相比，就只缺少MidKey这三个字段</strong>。</p><p>Root Index Block会在HFile解析的时候直接加载到内存中，此处需要注意在Trailer Block中有一个字段为dataIndexCount，就表示此处Index Entry的个数。因为Index Entry并不定长，只有知道Entry的个数才能正确的将所有Index Entry加载到内存。</p><h2><span id="nonroot-index-block">NonRoot Index Block</span></h2><p>当HFile中Data Block越来越多，single-level结构的索引已经不足以支撑所有数据都加载到内存，需要分化为mutil-level结构。mutil-level结构中NonRoot Index Block作为中间层节点或者叶子节点存在，无论是中间节点还是叶子节点，其都拥有相同的结构，如下图所示：</p><p><img src="/images/pasted-38.png" alt="upload successful"><br>和Root Index Block相同，NonRoot Index Block中最核心的字段也是Index Entry，用于指向叶子节点块或者数据块。<strong>不同的是，NonRoot Index Block结构中增加了block块的内部索引entry Offset字段</strong>，entry Offset表示index Entry在该block中的相对偏移量（相对于第一个index Entry)，用于实现block内的二分查找。所有非根节点索引块，包括Intermediate index block和leaf index block，在其内部定位一个key的具体索引并不是通过遍历实现，<strong>而是使用二分查找算法</strong>，这样可以更加高效快速地定位到待查找key。</p><h1><span id="hfile数据完整索引流程">HFile数据完整索引流程</span></h1><p>了解了HFile中数据索引块的两种结构之后，就来看看如何使用这些索引数据块进行数据的高效检索。整个索引体系类似于MySQL 的B+树结构，但是又有所不同，比B+树简单，并没有复杂的分裂操作。具体见下图所示：</p><p><img src="/images/pasted-39.png" alt="upload successful"></p><p>图中上面三层为索引层，<strong>在数据量不大的时候只有最上面一层，数据量大了之后开始分裂为多层，最多三层</strong>，如图所示。最下面一层为数据层，存储用户的实际keyvalue数据。这个索引树结构类似于InnoSQL的聚集索引，只是HBase并没有辅助索引的概念。  </p><p>图中红线表示一次查询的索引过程（HBase中相关类为HFileBlockIndex和HFileReaderV2）</p><p>基本流程可以表示为：</p><ol><li>用户输入rowkey为fb，在root index block中通过二分查找定位到fb在’a’和’m’之间，因此需要访问索引’a’指向的中间节点。因为root index block常驻内存，所以这个过程很快；<ol start="2"><li>将索引’a’指向的中间节点索引块加载到内存，然后通过二分查找定位到fb在index ‘d’和’h’之间，接下来访问索引’d’指向的叶子节点；</li><li>同理，将索引’d’指向的中间节点索引块加载到内存，一样通过二分查找定位找到fb在index ‘f’和’g’之间，最后需要访问索引’f’指向的数据块节点；</li><li>将索引’f’指向的数据块加载到内存，通过遍历的方式找到对应的keyvalue。</li></ol></li></ol><p>上述流程中因为<strong>中间节点、叶子节点和数据块都需要加载到内存，所以io次数正常为3次</strong>。但是实际上HBase为block提供了缓存机制，可以将频繁使用的block缓存在内存中，可以进一步加快实际读取过程。所以，在HBase中，通常一次随机读请求最多会产生3次io，如果数据量小（只有一层索引），数据已经缓存到了内存，就不会产生io。</p><h1><span id="索引块分裂">索引块分裂</span></h1><p>上文中已经提到，当数据量少、文件小的时候，只需要一个root index block就可以完成索引，即索引树只有一层。当数据不断写入，文件变大之后，索引数据也会相应变大，索引结构就会由single-level变为mulit-level，期间涉及到索引块的写入和分裂，本节来关注一下数据写入是如何引起索引块分裂的。   </p><p>如果大家之前看过HBase系列另一篇博文《HBase数据写入之Memstore Flush》，可以知道memstore flush主要分为3个阶段，  </p><ol><li>第一个阶段会讲memstore中的keyvalue数据snapshot，</li><li>第二阶段再将这部分数据flush的HFile，并生成在临时目录，</li><li>第三阶段将临时文件移动到指定的ColumnFamily目录下。   </li></ol><p>很显然，<strong>第二阶段将keyvalue数据flush到HFile将会是关注的重点</strong>（flush相关代码在DefaultStoreFlusher类中）。整个flush阶段又可以分为两阶段：</p><ol><li>append阶段：memstore中keyvalue首先会写入到HFile中数据块；</li><li>finalize阶段：修改HFlie中meta元数据块，索引数据块以及Trailer数据块等。</li></ol><h2><span id="append流程">Append流程</span></h2><p>具体key value数据的append以及finalize过程在HFileWriterV2文件中，其中append流程可以大体表征为：</p><p><img src="/images/pasted-40.png" alt="upload successful"></p><ol><li>预检查：检查key的大小是否大于前一个key，如果大于则不符合HBase顺序排列的原理，抛出异常；检查value是否是null，如果为null也抛出异常；</li><li>block是否写满：检查当前Data Block是否已经写满，如果没有写满就直接写入keyvalue；否则就需要执行数据块落盘以及索引块修改操作；</li><li><strong>数据落盘并修改索引</strong>：如果DataBlock写满，首先将block块写入流；<strong>再生成一个leaf index entry，写入leaf Index block</strong>；再检查该leaf index block是否已经写满需要落盘，如果已经写满，就将该leaf index block写入到输出流，并且为索引树根节点root index block新增一个索引，指向叶子节点(second-level index)；</li><li>生成一个新的block：重新reset输出流，初始化startOffset为-1； </li><li>写入keyvalue：将keyvalue以流的方式写入输出流，同时需要写入memstoreTS；除此之外，如果该key是当前block的第一个key，需要赋值给变量firstKeyInBlock。</li></ol><h2><span id="finalize阶段">Finalize阶段</span></h2><p>memstore中所有keyvalue都经过append阶段输出到HFile后，会执行一次finalize过程，主要更新HFile中meta元数据块、索引数据块以及Trailer数据块，其中对索引数据块的更新是我们关心的重点，此处详细解析，上述append流程中c步骤’数据落盘并修改索引’会使得root index block不断增多，当增大到一定程度之后就需要分裂，分裂示意图如下图所示：</p><p><img src="/images/pasted-41.png" alt="upload successful"><br>上图所示，分裂前索引结构为second-level结构，图中没有画出Data Blocks，根节点索引指向叶子节点索引块。finalize阶段系统会对Root Index Block进行大小检查，如果大小大于规定的大小就需要进行分裂，图中分裂过程实际上就是将原来的Root Index Block块分割成4块，每块独立形成中间节点InterMediate Index Block，系统再重新生成一个Root Index Block（图中红色部分），分别指向分割形成的4个interMediate Index Block。此时索引结构就变成了third-level结构。</p><h1><span id="总-结">总 结</span></h1><p>这篇文章是HFile结构解析的第二篇文章，主要集中介绍HFile中的数据索引块。<br>①首先分Root Index Block和NonRoot Index Block 两部分对HFile中索引块进行了解析，<br>②紧接着基于此介绍了HBase如何使用索引对数据进行检索，<br>③最后结合Memstore Flush的相关知识分析了keyvalue数据写入的过程中索引块的分裂过程。   </p><p>希望通过这两篇文章的介绍，能够对HBase中数据存储文件HFile有一个更加全面深入的认识。</p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase之HFile索引机制一HFile存储结构</title>
      <link href="/2019/04/26/HBase%E4%B9%8BHFile%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/04/26/HBase%E4%B9%8BHFile%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#hfile演化">Hfile演化</a></li><li><a href="#hfile-v2的逻辑结构">HFile V2的逻辑结构</a></li><li><a href="#hfile-v2物理结构">HFile v2物理结构</a></li><li><a href="#hfile-v2中核心block块解析">HFile V2中核心Block块解析</a><ul><li><a href="#trailer-block">Trailer Block</a></li><li><a href="#data-block">Data Block</a></li><li><a href="#bloomfilter-meta-block-bloom-block">BloomFilter Meta Block &amp; Bloom Block</a></li></ul></li><li><a href="#总结">总结</a></li></ul><!-- tocstop --><h1><span id="hfile演化">Hfile演化</span></h1><hr><p>HFile是HBase存储数据的文件组织形式，参考BigTable的SSTable和Hadoop的TFile实现。从HBase开始到现在，HFile经历了三个版本，其中V2在0.92引入，V3在0.98引入。HFileV1版本的在实际使用过程中发现它占用内存多，HFile V2版本针对此进行了优化，HFile V3版本基本和V2版本相同，只是在cell层面添加了Tag数组的支持。鉴于此，<strong>本文主要针对V2版本进行分析</strong>，对V1和V3 版本感兴趣的同学可以参考其他信息。    </p><h1><span id="hfile-v2的逻辑结构">HFile V2的逻辑结构</span></h1><hr><p><img src="/images/pasted-26.png" alt="upload successful"><br><img src="/images/pasted-42.png" alt="upload successful"><br>文件主要分为四个部分：</p><ol><li>Scanned block section<br>顾名思义，表示顺序扫描HFile时所有的数据块将会被读取，包括Leaf Index Block和Bloom Block。</li><li>Non-scanned block section<br>表示在HFile顺序扫描的时候数据不会被读取，主要包括Meta Block和Intermediate Level Data Index Blocks两部分。</li><li>Load-on-open-section<br><strong>这部分数据在HBase的region server启动时，需要加载到内存中</strong>。包括root data block index和meta data block index、FileInfo、Bloom filter block。    </li><li>Trailer<br>这部分主要记录了HFile的基本信息、各个部分的偏移值和寻址信息。    <h1><span id="hfile-v2物理结构">HFile v2物理结构</span></h1></li></ol><hr><p><img src="/images/pasted-27.png" alt="upload successful"></p><p>如上图所示，HFlie会被切分为多个大小相等的block块，每个block的大小可以在创建表列簇的时候通过参数blocksize=&gt;’65535‘进行指定，默认为64K，大号的Block有利于顺序Scan，小号Block利于随机查询，因而需要权衡。<br>而且所有block块都拥有相同的数据结构，如图左侧所示，HBase将block块抽象为一个统一的HFileBlock。HFileBlock支持两种类型，一种类型不支持checksum，一种支持。<br>为方便讲解，下图选用<strong>不支持checksum的HFileBlock内部结构</strong>：</p><p><img src="/images/pasted-28.png" alt="upload successful"><br>上图所示HFileBlock主要包括两部分：</p><ol><li>BlockHeader主要存储block元数据<br>block元数据中最核心的字段是BlockType字段，用来标示该block块的类型，HBase中定义了8种BlockType，每种BlockType对应的block都存储不同的数据内容，有的存储用户数据，有的存储索引数据，有的存储meta元数据。对于任意一种类型的HFileBlock， 都拥有相同结构的BlockHeader </li><li>BlockData用来存储具体数据。<br>但是BlockData结构却不相同。下面通过一张表简单罗列最核心的几种BlockType，下文会详细针对每种BlockType进行详细的讲解：<br><img src="/images/pasted-29.png" alt="upload successful"><h1><span id="hfile-v2中核心block块解析">HFile V2中核心Block块解析</span></h1></li></ol><hr><p>上文从HFile的层面将文件切分成了多种类型的block，接下来针对几种重要block进行详细的介绍，因为篇幅的原因，索引相关的block不会在本文进行介绍，接下来会写一篇单独的文章对其进行分析和讲解。①首先会介绍记录HFile基本信息的TrailerBlock，②再介绍用户数据的实际存储块DataBlock，③最后简单介绍布隆过滤器相关的block。   </p><h2><span id="trailer-block">Trailer Block</span></h2><hr><p>TrailerBlock主要记录了HFile的基本信息、各个部分的偏移值和寻址信息，下图为Trailer内存和磁盘中的数据结构，其中只显示了部分核心字段：<br><img src="/images/pasted-30.png" alt="upload successful"><br>HFile在读取的时候<strong>首先会解析Trailer Block并加载到内存，然后再进一步加载LoadOnOpen区的数据</strong>，具体步骤如下：</p><ol><li>首先加载version版本信息，HBase中version包含majorVersion和minorVersion两部分，①前者决定了HFile的主版本：V1、V2 还是V3；②后者在主版本确定的基础上决定是否支持一些微小修正，比如是否支持checksum等。<strong>不同的版本决定了使用不同的Reader对象对HFile进行读取解析</strong></li><li>根据Version信息获取trailer的长度（不同version的trailer长度不同），再根据trailer长度加载整个HFileTrailer Block   </li><li><strong>最后加载load-on-open部分到内存中，①起始偏移地址是trailer中的LoadOnOpenDataOffset字段，load-on-open部分的结束偏移量为HFile长度减去Trailer长度，②load-on-open部分主要包括索引树的根节点以及Filelnfo两个重要模块，Filelnfo是固定长度的block块 ，它纪录了文的一些Meta信息</strong>，例 如 ： AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等；索引树根节点放到下一篇文章进行介绍。</li></ol><h2><span id="data-block">Data Block</span></h2><hr><p>DataBlock 是HBase 中数据存储的最小单元。DataBlock 中主要存储用户的KeyValue 数据（ KeyValue 后面一般会跟一个timestamp ， 图中未标出）， 而KeyValue结构是HBase存储的核心， 每个数据都是以KeyValue结构在HBase中进行存储。KeyValue结构在内存和磁盘中可以表示为：<br><img src="/images/pasted-31.png" alt="upload successful"><br>每个KeyValue都由4个部分构成，分别为key length，value length，key和value。</p><ol><li>key length和value length是两个固定长度的数值</li><li>key是一个复杂的结构<br>①首先是rowkey的长度<br>②接着是rowkey<br>③然后是ColumnFamily的长度<br>④再是ColumnFamily<br>⑤最后是时间戳和KeyType（keytype有四种类型，分别是Put、Delete、 DeleteColumn和DeleteFamily）</li><li>key是一个复杂的结构①首先是rowkey的长度，②接着是rowkey，③然后是ColumnFamily的长度，再是ColumnFamily， ④最后是时间戳和KeyType（keytype有四种类型，分别是Put、Delete、 DeleteColumn和DeleteFamily）</li><li>value就没有那么复杂，就是一串纯粹的二进制数据。</li></ol><h2><span id="bloomfilter-meta-block-amp-bloom-block">BloomFilter Meta Block &amp; Bloom Block</span></h2><p><strong>BloomFilter Meta Block在Load-on-open-section中，而Bloom Block是在Scanned block section中</strong>。<br><strong>BloomFilter对于HBase的随机读性能至关重要</strong>，对于get操作以及部分scan操作可以剔除掉不会用到的HFile文件，减少实际IO次数，提高随机读性能。    </p><p>在此简单地介绍一下Bloom Filter的工作原理，Bloom Filter使用位数组来实现过滤，初始状态下位数组每一位都为0，如下图所示：</p><p><img src="/images/pasted-32.png" alt="upload successful"><br>假如此时有一个集合S = {x1, x2, … xn}，Bloom Filter使用<strong>k个独立的hash函数</strong>，分别将集合中的每一个元素映射到｛1,…,m｝的范围。<strong>对于任何一个元素，被映射到的数字作为对应的位数组的索引，该位会被置为1</strong>。比如元素x1被hash函数映射到数字8，那么位数组的第8位就会被置为1。下图中集合S只有两个元素x和y，分别被3个hash函数进行映射，映射到的位置分别为（0，3，6）和（4，7，10），对应的位会被置为1:   </p><p><img src="/images/pasted-33.png" alt="upload successful"><br>现在假如要判断另一个元素是否是在此集合中，只需要被这3个hash函数进行映射，查看对应的位置是否有0存在，如果有的话，表示此元素肯定不存在于这个集合，否则有可能存在。下图所示就表示z肯定不在集合｛x，y｝中：</p><p><img src="/images/pasted-34.png" alt="upload successful"><br><strong>Bloom Filter只要认为不存在就一定不存在，认为存在会可能不存在</strong>    </p><p>HBase中每个HFile都有对应的位数组，<strong>KeyValue在写入HFile时会先经过几个hash函数的映射，映射后将对应的数组位改为1</strong>， get请求进来之后再进行hash映射，如果在对应数组位上存在0，说明该get请求查询的数据不在该HFile中。    </p><p>HFile中的位数组就是上述Bloom Block中存储的值，可以想象，一个HFile文件越大，里面存储的KeyValue值越多，位数组就会相应越大。一旦太大就不适合直接加载到内存了，<strong>因此HFile V2在设计上将位数组进行了拆分，拆成了多个独立的位数组（根据Key进行拆分，一部分连续的Key使用一个位数组）。这样一个HFile中就会包含多个位数组，根据Key进行查询，首先会定位到具体的某个位数组，只需要加载此位数组到内存进行过滤即可，减少了内存开支</strong>。    </p><p><strong>在结构上每个位数组对应HFile中一个Bloom Block，为了方便根据Key定位具体需要加载哪个位数组，HFile V2又设计了对应的索引Bloom Index Block</strong>，对应的内存和逻辑结构图如下：</p><p><img src="/images/pasted-35.png" alt="upload successful"><br>Bloom Index Block结构中</p><ol><li>totalByteSize表示位数组的bit数</li><li>numChunks表示Bloom Block的个数</li><li>hashCount表示hash函数的个数</li><li>hashType表示hash函数的类型</li><li>totalKeyCount表示bloom filter当前已经包含的key的数目</li><li>totalMaxKeys表示bloomfilter当前最多包含的key的数目</li><li>Bloom Index Entry对应每一个bloom filter block的索引条目，<strong>作为索引分别指向’scanned block section’部分的Bloom Block</strong>，Bloom Block中就存储了对应的位数组。</li></ol><p>Bloom Index Entry 的结构见上图左边所示</p><ol><li>BlockOffset 表示对应Bloom Block 在HFile 中的偏移量</li><li>FirstKey 表示对应BloomBlock的第一个Key。</li><li>根据上文所说，一次get请求进来，首先会根据key在所有的索引条目中进行二分查找，查找到对应的Bloom Index Entry，就可以定位到该key对应的位数组，加载到内存进行过滤判断。<h1><span id="总结">总结</span></h1>这篇小文<br>①首先从宏观的层面对HFile的逻辑结构和物理存储结构进行了讲解，<br>②并且将HFile从逻辑上分解为各种类型的Block，<br>③再接着从微观的视角分别对Trailer Block、Data Block在结构上进行了解析：通过对Trailer Block的解析，可以获取HFile的版本以及HFile中其他几个部分的偏移量，在读取的时候可以直接通过偏移量对其进行加载；<br>④而对Data Block的解析可以知道用户数据在HDFS中是如何实际存储的；<br>⑤最后通过介绍Bloom Filter的工作原理以及相关的Block块了解HFile中Bloom Filter的存储结构。</li></ol><p>接下来会以本文为基础，再写一篇文章分析HFile中索引块的结构以及相应的索引机制。</p><p>参考：<br><a href="http://www.cnblogs.com/163yun/p/9020629.html" target="_blank" rel="noopener">文件HFile存储结构解析</a><br><a href="https://www.cnblogs.com/163yun/p/9020539.html" target="_blank" rel="noopener">HBase – 探索HFile索引机制</a></p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Region寻址</title>
      <link href="/2019/04/26/Region%E5%AF%BB%E5%9D%80/"/>
      <url>/2019/04/26/Region%E5%AF%BB%E5%9D%80/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#096版本以前的寻址方式">0.96版本以前的寻址方式</a></li><li><a href="#新的region寻址方式">新的Region寻址方式</a></li><li><a href="#总结">总结</a></li></ul><!-- tocstop --><h1><span id="096版本以前的寻址方式">0.96版本以前的寻址方式</span></h1><hr><p>在Hbase 0.96版本以前，Hbase有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT-的位置存储在ZooKeeper中，-ROOT-本身存储了 .META. Table的RegionInfo信息，<strong>并且-ROOT-不会分裂，只有一个region。而.META.表可以被切分成多个region</strong>。读取的流程如下图所示：</p><p><img src="/images/pasted-23.png" alt="upload successful"></p><p><img src="/images/pasted-24.png" alt="upload successful"><br>寻址步骤： </p><ol><li>client请求ZK获得-ROOT-所在的RegionServer地址   </li><li>client请求-ROOT-所在的RS地址，获取.META.表的地址，client会将-ROOT-的相关信息cache下来，以便下一次快速访问    </li><li>client请求 .META.表的RS地址，获取访问数据所在RegionServer的地址，client会将.META.的相关信息cache下来，以便下一次快速访问    </li><li>client请求访问数据所在RegionServer的地址，获取对应的数据</li></ol><p>从上面的路径我们可以看出，用户需要3次请求才能直到用户Table真正的位置，然后第4次请求开始获取真正的数据,这在一定程序带来了性能的下降。<strong>即最差要四次请求</strong>    </p><p>在0.96之前使用3层设计的主要原因是考虑到元数据可能需要很大。但是真正集群运行，元数据的大小其实很容易计算出来。在BigTable的论文中，每行METADATA数据存储大小为1KB左右，如果按照一个Region为128M的计算，3层设计可以支持的Region个数为2^34个，采用2层设计可以支持2^17（131072）。  </p><p>分析一：<br>如果一个region的大小为128m,那么就说明.META.的大小也为128M。每行METADATA数据存储大小为1KB左右，那么.META.表就可以存储<br>131072 KB = 128M =2^17个region. 此时按照每个region 128m大小，131072*128M = 16T.而且<del>meta表肯定不止一个region，</del>一个region肯定不止128M，所以，从meta来定位的数据大小远大于16T。<br>如果单个region大小设置为2G，那么此时一个meta region就可以容纳4P的数据。<br>分析二：<br>为什么3层设计可以支持的Region个数为2^34个，采用2层设计可以支持2^17？<br>3层则是-ROOT-表128M，其中每1kb的数据存储一张.META.表，总可以存128*1024*128张.meta表即2^34张.   </p><h1><span id="新的region寻址方式">新的Region寻址方式</span></h1><hr><p>如上面的计算，2层结构其实完全能满足业务的需求，因此0.96版本以后将-ROOT-表去掉了。如下图所示：</p><p><img src="/images/pasted-25.png" alt="upload successful"><br>访问步骤：</p><ol><li>Client请求ZK获取.META.所在的RegionServer的地址。   </li><li>Client请求.META.所在的RegionServer获取访问数据所在的RegionServer地址，client会将.META.的相关信息cache下来，以便下一次快速访问。   </li><li>Client请求数据所在的RegionServer，获取所需要的数据。</li></ol><p><strong>访问路径变成了3步</strong>    </p><h1><span id="总结">总结</span></h1><p>总结去掉-ROOT-的原因有如下2点：<br>其一：提高性能<br>其二：2层结构已经足以满足集群的需求<br>这里还有一个问题需要说明，那就是Client会缓存.META.的数据，用来加快访问，既然有缓存，那它什么时候更新？如果.META.更新了，比如Region1不在RerverServer2上了，被转移到了RerverServer3上。client的缓存没有更新会有什么情况？<br>其实，Client的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于Region1的位置发生了变化，Client再次根据缓存去访问的时候，会出现错误，当出现异常达到重试次数后就会去.META.所在的RegionServer获取最新的数据，如果.META.所在的RegionServer也变了，Client就会去ZK上获取.META.所在的RegionServer的最新地址。</p><p>参考：<br><a href="https://blog.csdn.net/qq_36421826/article/details/82701677" target="_blank" rel="noopener">https://blog.csdn.net/qq_36421826/article/details/82701677</a><br><a href="https://www.cnblogs.com/qcloud1001/p/7615526.html" target="_blank" rel="noopener">https://www.cnblogs.com/qcloud1001/p/7615526.html</a><br><a href="https://www.cnblogs.com/ios1988/p/6266767.html" target="_blank" rel="noopener">https://www.cnblogs.com/ios1988/p/6266767.html</a><br><a href="https://www.cnblogs.com/cenzhongman/p/7271761.html" target="_blank" rel="noopener">https://www.cnblogs.com/cenzhongman/p/7271761.html</a></p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基本数据结构一</title>
      <link href="/2019/04/26/%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%80/"/>
      <url>/2019/04/26/%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#数据结构" target="_blank" rel="noopener">数据结构</a><ul><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#队列" target="_blank" rel="noopener">队列</a></li><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#集合" target="_blank" rel="noopener">集合</a></li><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#链表数组" target="_blank" rel="noopener">链表、数组</a></li><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#字典关联数组" target="_blank" rel="noopener">字典、关联数组</a></li><li><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#栈" target="_blank" rel="noopener">栈</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基本数据结构三-BitSet</title>
      <link href="/2019/04/26/BitSet/"/>
      <url>/2019/04/26/BitSet/</url>
      
        <content type="html"><![CDATA[<p>BitSet是位操作的对象，值只有0或1即false和true，内部维护了一个long数组，初始只有一个long，所以BitSet最小的size是64，当随着存储的元素越来越多，BitSet内部会动态扩充，一次扩充64位，最终内部是由N个long来存储。</p><p>默认情况下，BitSet的所有位都是false即0。</p><p>(Java Bitset类)[<a href="http://www.runoob.com/java/java-bitset-class.html]" target="_blank" rel="noopener">http://www.runoob.com/java/java-bitset-class.html]</a><br>(Java BitSet位集)[<a href="https://blog.csdn.net/caiandyong/article/details/51581160]" target="_blank" rel="noopener">https://blog.csdn.net/caiandyong/article/details/51581160]</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">void set(int index, boolean v)</span><br><span class="line">将指定索引处的位设置为指定的值。</span><br><span class="line">void set(int index)</span><br><span class="line">将指定索引处的位设置为 true。</span><br></pre></td></tr></table></figure></p><p>实例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.BitSet;</span><br><span class="line"> </span><br><span class="line">public class BitSetDemo &#123;</span><br><span class="line"> </span><br><span class="line">  public static void main(String args[]) &#123;</span><br><span class="line">     BitSet bits1 = new BitSet(16);</span><br><span class="line">     BitSet bits2 = new BitSet(16);</span><br><span class="line">      </span><br><span class="line">     // set some bits</span><br><span class="line">     for(int i=0; i&lt;16; i++) &#123;</span><br><span class="line">        if((i%2) == 0) bits1.set(i);</span><br><span class="line">        if((i%5) != 0) bits2.set(i);</span><br><span class="line">     &#125;</span><br><span class="line">     System.out.println(&quot;Initial pattern in bits1: &quot;);</span><br><span class="line">     System.out.println(bits1);</span><br><span class="line">     System.out.println(&quot;\nInitial pattern in bits2: &quot;);</span><br><span class="line">     System.out.println(bits2);</span><br><span class="line"> </span><br><span class="line">     // AND bits</span><br><span class="line">     bits2.and(bits1);</span><br><span class="line">     System.out.println(&quot;\nbits2 AND bits1: &quot;);</span><br><span class="line">     System.out.println(bits2);</span><br><span class="line"> </span><br><span class="line">     // OR bits</span><br><span class="line">     bits2.or(bits1);</span><br><span class="line">     System.out.println(&quot;\nbits2 OR bits1: &quot;);</span><br><span class="line">     System.out.println(bits2);</span><br><span class="line"> </span><br><span class="line">     // XOR bits</span><br><span class="line">     bits2.xor(bits1);</span><br><span class="line">     System.out.println(&quot;\nbits2 XOR bits1: &quot;);</span><br><span class="line">     System.out.println(bits2);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Initial pattern in bits1:</span><br><span class="line">&#123;0, 2, 4, 6, 8, 10, 12, 14&#125;</span><br><span class="line"></span><br><span class="line">Initial pattern in bits2:</span><br><span class="line">&#123;1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14&#125;</span><br><span class="line"></span><br><span class="line">bits2 AND bits1:</span><br><span class="line">&#123;2, 4, 6, 8, 12, 14&#125;</span><br><span class="line"></span><br><span class="line">bits2 OR bits1:</span><br><span class="line">&#123;0, 2, 4, 6, 8, 10, 12, 14&#125;</span><br><span class="line"></span><br><span class="line">bits2 XOR bits1:</span><br><span class="line">&#123;&#125;</span><br></pre></td></tr></table></figure></p><p>通常不直接使用原生的bitmap，而使用RoaringBitmap，它的压缩比更好<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void test3() throws IOException &#123;</span><br><span class="line">       RoaringBitmap r1 = new RoaringBitmap();</span><br><span class="line">   r1.add(1);//表示将索引为1的位置置为1</span><br><span class="line">   r1.add(0);//表示将索引为0的位置置为1</span><br><span class="line">   r1.add(6);//表示将索引为6的位置置为1</span><br><span class="line">       RoaringBitmap r2 = new RoaringBitmap();</span><br><span class="line">       r2.add(0);//表示将索引为1的位置置为1</span><br><span class="line">       r2.add(6);//表示将索引为6的位置置为1</span><br><span class="line"></span><br><span class="line">   RoaringBitmap and = RoaringBitmap.and(r1, r2);</span><br><span class="line">   System.out.println(and.toString());//得到的是共同元素索引的位置</span><br><span class="line">   System.out.println(and.getCardinality());</span><br></pre></td></tr></table></figure></p><p>druid也是使用该方式实现的底层模块。<br>(使用方案)[<a href="https://juejin.im/post/5b4ea9625188251b381269e6]" target="_blank" rel="noopener">https://juejin.im/post/5b4ea9625188251b381269e6]</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基本数据结构二-树</title>
      <link href="/2019/04/26/%E6%A0%91/"/>
      <url>/2019/04/26/%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#为什么要有二叉树">为什么要有二叉树?</a></li><li><a href="#完全二叉树">完全二叉树</a></li><li><a href="#二叉查找树bst">二叉查找树(BST)</a></li><li><a href="#平衡二叉树">平衡二叉树</a><ul><li><a href="#2-3查找树2-3-search-tree">2-3查找树(2-3 Search Tree)</a></li></ul></li><li><a href="#红黑树">红黑树</a></li><li><a href="#b树b树b树">B树，B+树，B*树</a><ul><li><a href="#b树是一种多路搜索树并不一定是二叉的">B树是一种多路搜索树（并不一定是二叉的）</a></li><li><a href="#b树">B+树</a><ul><li><a href="#b树-vs-b树">B+树 VS B树</a></li></ul></li><li><a href="#b树">B*树</a></li><li><a href="#小结">小结</a></li></ul></li><li><a href="#lsm树">LSM树</a><ul><li><a href="#复习b树">复习B+树</a></li><li><a href="#lsm树-1">LSM树</a></li><li><a href="#lsm在hbase中的使用">LSM在HBase中的使用</a></li></ul></li></ul><!-- tocstop --><p>树的概念咱们就直接跳过了。    </p><h1><span id="为什么要有二叉树">为什么要有二叉树?</span></h1><hr><p>在实际使用时会根据链表和有序数组等数据结构的不同优势进行选择。<br><strong>优点</strong>：有序数组的优势在于二分查找，链表的优势在于数据项的插入和数据项的删除。<br><strong>缺点</strong>：在有序数组中插入数据就会很慢，同样在链表中查找数据项效率就很低。<br><strong>综合</strong>：二叉树可以利用链表和有序数组的优势，同时可以合并有序数组和链表的优势，二叉树也是一种常用的数据结构。   </p><p>二叉树的构成： 二叉树由节点（node）和边组成。节点分为根节点、父节点、子节点。二叉树每个节点最多有两个叶子节点。  </p><h1><span id="完全二叉树">完全二叉树</span></h1><hr><p>叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。</p><h1><span id="二叉查找树bst">二叉查找树(BST)</span></h1><hr><p>Binary Search Tree），也称有序二叉树（ordered binary tree）,排序二叉树（sorted binary tree）<br>普通二叉树的每个节点的左子节点的关键字小于这个节点，右子节点关键字大于或等于这个父节点。     </p><h1><span id="平衡二叉树">平衡二叉树</span></h1><hr><p>左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。<br>平衡二叉树（Balanced Binary Tree),又叫AVL树.<br>平衡二叉树是二叉查找树的一个进化体，也是第一个引入平衡概念的二叉树，所以说平衡二叉树首先是二叉查找树。<br>优点：这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在O(logN)<br>缺点：但是频繁旋转会使插入和删除牺牲掉O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。    </p><h2><span id="2-3查找树2-3-search-tree">2-3查找树(2-3 Search Tree)</span></h2><p>平衡二叉树每次插入元素之后维持树的平衡状态太昂贵。所以这里会介绍一些新的数据结构来保证在最坏的情况下插入和查找效率都能保证在<strong>对数的时间复杂度内</strong>完成。<br><a href="http://www.cnblogs.com/yangecnu/p/Introduce-2-3-Search-Tree.html" target="_blank" rel="noopener">2-3查找树</a></p><h1><span id="红黑树">红黑树</span></h1><p><a href="https://blog.csdn.net/sun_tttt/article/details/65445754" target="_blank" rel="noopener"><br>最容易懂得红黑树</a><br><a href="http://www.cnblogs.com/yangecnu/p/Introduce-Red-Black-Tree.html" target="_blank" rel="noopener">浅谈算法和数据结构: 九 平衡查找树之红黑树</a></p><h1><span id="b树b树b树">B树，B+树，B*树</span></h1><p>MySQL是基于B+树聚集索引组织表<br><a href="https://blog.csdn.net/aqzwss/article/details/53074186" target="_blank" rel="noopener">B树，B+树，B*树</a></p><h2><span id="b树是一种多路搜索树并不一定是二叉的">B树是一种多路搜索树（并不一定是二叉的）</span></h2><p>一棵m阶B树(balanced tree of order m)是一棵平衡的m路搜索树。或者是空树，或者是满足特定性质的树。 </p><p><img src="/images/pasted-18.png" alt="upload successful"><br>B树的搜索，从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为空，或已经是叶子结点；</p><p>B-树的特性：  </p><ol><li>关键字集合<strong>分布在整颗树中</strong>；</li><li>任何一个关键字出现且<strong>只出现在一个结点中</strong>；</li><li>搜索<strong>有可能在非叶子结点结束</strong>；</li><li>其搜索性能等价于在关键字全集内做一次二分查找；</li><li>自动层次控制；<h2><span id="b树">B+树</span></h2>B+ 树是一种树数据结构，是一个n叉树，每个节点通常有多个孩子，一棵B+树包含根节点、内部节点和叶子节点。根节点可能是一个叶子节点，也可能是一个包含两个或两个以上孩子节点的节点。<br>用途：<br>B+ 树通常用于数据库和操作系统的文件系统中。NTFS, ReiserFS, NSS, XFS, JFS, ReFS 和BFS等文件系统都在使用B+树作为元数据索引。B+ 树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+ 树元素自底向上插入。<br>B+树是B-树的变体，也是一种多路搜索树。   <h3><span id="b树-vs-b树">B+树 VS B树</span></h3></li><li>有n棵子树的结点中含有n个关键字，<strong>每个关键字不保存数据，只用来索引，所有数据都保存在叶子节点</strong>。</li><li>所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。</li><li><strong>所有的非终端结点可以看成是索引部分</strong>，结点中仅含其子树（根结点）中的最大（或最小）关键字。<br>通常在B+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。</li></ol><p><img src="/images/pasted-19.png" alt="upload successful"><br>B+的搜索与B-树也基本相同，区别是B+树只有达到叶子结点才命中（B-树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找；    </p><p>B+的特性：</p><ol><li>所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的；</li><li>不可能在非叶子结点命中；</li><li>非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层；</li><li>更适合文件索引系统；<h2><span id="b树">B*树</span></h2>是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针；  </li></ol><p><img src="/images/pasted-20.png" alt="upload successful"></p><h2><span id="小结">小结</span></h2><p>B-树：<br>多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点；所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中；</p><p>B+树：<br>在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；</p><p>B*树：<br>在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3；</p><h1><span id="lsm树">LSM树</span></h1><p>LSM（Log-Structured Merge-Trees）和 B+ 树相比，是牺牲了部分读的性能来换取写的性能(通过批量写入)，实现读写之间的。 Hbase、LevelDB、Tair（Long DB）、nessDB 采用 LSM 树的结构。LSM可以快速建立索引。<br><a href="https://blog.csdn.net/dbanote/article/details/8897599" target="_blank" rel="noopener">[HBase] LSM树 VS B+树</a></p><h2><span id="复习b树">复习B+树</span></h2><p>比如Oracle的普通索引就是采用B+树的方式，下面是一个B+树的例子：   </p><p><img src="/images/pasted-21.png" alt="upload successful"><br>根节点和枝节点很简单，分别记录每个叶子节点的最小值(节点红色区域)，并用一个指针指向叶子节点(节点黄色区域)。E节点会存在多层多次，且每个节点分为红色和黄色两部分。   </p><p>叶子节点里每个键值都指向真正的数据块（如Oracle里的RowID），每个叶子节点都有前指针(节点红色区域)和后指针(节点红色区域)，这是为了做范围查询时，叶子节点间可以直接跳转，从而避免再去回溯至枝和跟节点。<br>缺点：<br>B+树最大的性能问题是会产生大量的随机IO，随着新数据的插入，叶子节点会慢慢分裂，逻辑上连续的叶子节点在物理上往往不连续，甚至分离的很远，但做范围查询时，会产生大量读随机IO。</p><p>对于大量的随机写也一样，举一个插入key跨度很大的例子，如7-&gt;1000-&gt;3-&gt;2000 … 新插入的数据存储在磁盘上相隔很远，会产生大量的随机写IO.</p><p>从上面可以看出，低下的磁盘寻道速度严重影响性能（近些年来，磁盘寻道速度的发展几乎处于停滞的状态）。</p><h2><span id="lsm树">LSM树</span></h2><p>为了克服B+树的弱点，HBase引入了LSM树的概念，即Log-Structured Merge-Trees。</p><p>为了更好的说明LSM树的原理，下面举个比较极端的例子：</p><p>现在假设有1000个节点的随机key，对于磁盘来说，肯定是把这1000个节点顺序写入磁盘最快，但是这样一来，读就悲剧了，因为key在磁盘中完全无序，每次读取都要全扫描；</p><p>那么，为了让读性能尽量高，数据在磁盘中必须得有序，这就是B+树的原理，但是写就悲剧了，因为会产生大量的随机IO，磁盘寻道速度跟不上。</p><p>LSM树本质上就是在读写之间取得平衡，和B+树相比，它牺牲了部分读性能，用来大幅提高写性能。</p><p>它的原理是把一颗大树拆分成N棵小树， 它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。当读时，由于不知道数据在哪棵小树上，因此必须遍历所有的小树，但在每颗小树内部数据是有序的。</p><p><img src="/images/pasted-22.png" alt="upload successful"></p><h2><span id="lsm在hbase中的使用">LSM在HBase中的使用</span></h2><p>以上就是LSM树最本质的原理，有了原理，再看具体的技术就很简单了。</p><ol><li>首先说说为什么要有WAL（Write Ahead Log），很简单，因为数据是先写到内存中，如果断电，内存中的数据会丢失，因此为了保护内存中的数据，需要在磁盘上先记录logfile，当内存中的数据flush到磁盘上时，就可以抛弃相应的Logfile。</li><li>什么是memstore, storefile？很简单，上面说过，LSM树就是一堆小树，在内存中的小树即memstore，每次flush，内存中的memstore变成磁盘上一个新的storefile。</li><li>为什么会有compact？很简单，随着小树越来越多，读的性能会越来越差，因此需要在适当的时候，对磁盘中的小树进行merge，多棵小树变成一颗大树。</li></ol><p><a href="https://blog.csdn.net/u014774781/article/details/52105708" target="_blank" rel="noopener">LSM树（Log-Structured Merge Tree）存储引擎</a><br>极端的说，基于LSM树实现的HBase的写性能比MySQL高了一个数量级，读性能低了一个数量级。<br>优化方式：Bloom filter 替代二分查找；compact 小数位大树，提高查询性能。<br>Hbase 中，内存中达到一定阈值后，整体flush到磁盘上、形成一个文件（B+数），HDFS不支持update操作，所以Hbase做整体flush而不是merge update。flush到磁盘上的小树，定期会合并成一个大树。    </p><p>参考：<br><a href="https://github.com/xingshaocheng/architect-awesome/blob/master/README.md#%E6%A0%91" target="_blank" rel="noopener">后端架构师技术图谱</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Druid Storage 原理二</title>
      <link href="/2019/04/25/Druid-Storage-%E5%8E%9F%E7%90%86%E4%BA%8C/"/>
      <url>/2019/04/25/Druid-Storage-%E5%8E%9F%E7%90%86%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#segment生成过程">Segment生成过程</a></li><li><a href="#segment-load过程">Segment load过程</a></li><li><a href="#segment-query过程">Segment Query过程</a></li><li><a href="#druid的编码和压缩">Druid的编码和压缩</a></li></ul><!-- tocstop --><h1><span id="segment生成过程">Segment生成过程</span></h1><hr><ol><li>Add Row to Map</li><li>Begin persist to disk</li><li>Write version file</li><li>Merge and write dimension dict</li><li>Write time column</li><li>Write metric column</li><li>Write dimension column</li><li>Write index.drd</li><li>Merge and write bitmaps</li><li>Write metadata.drd<h1><span id="segment-load过程">Segment load过程</span></h1></li></ol><hr><p><img src="/images/pasted-17.png" alt="upload successful"></p><ol><li>Read version</li><li>Load segment to MappedByteBuffer</li><li>Get column offset from meta</li><li>Deserialize each column from ByteBuffer</li></ol><h1><span id="segment-query过程">Segment Query过程</span></h1><hr><p>Druid查询的最小单位是Segment，Segment在查询之前必须先load到内存，load过程如上一步所述。如果没有索引的话，我们的查询过程就只能Scan的，遇到符合条件的行选择出来，但是所有查询都进行全表Scan肯定是不可行的，所以我们需要索引来快速过滤不需要的行。Druid的Segmenet查询过程如下：</p><ol><li>构造1个Cursor进行迭代</li><li>查询之前构造出Fliter</li><li>根据Index匹配Fliter，得到满足条件的Row的Offset</li><li>根据每列的ColumnSelector去指定Row读取需要的列。<h1><span id="druid的编码和压缩">Druid的编码和压缩</span></h1></li></ol><hr><p>前面已经提到了，Druid对Long型的指标进行了差分编码和Table编码，Long型和Float型的指标进行了LZ4或者LZF压缩。</p><p>其实编码和压缩本质上是一个东西，一切熵增的编码都是压缩。 在计算机领域，我们一般把针对特定类型的编码称之为编码，针对任意类型的通用编码称之为压缩。</p><p>编码和压缩的本质就是让每一个bit尽可能带有更多的信息。</p>]]></content>
      
      
      <categories>
          
          <category> druid </category>
          
      </categories>
      
      
        <tags>
            
            <tag> druid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Druid Storage 原理一</title>
      <link href="/2019/04/25/Druid-Storage-%E5%8E%9F%E7%90%86/"/>
      <url>/2019/04/25/Druid-Storage-%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#what-is-druid">What is Druid</a></li><li><a href="#druid架构">Druid架构</a><ul><li><a href="#节点分类">节点分类</a><ul><li><a href="#druid">druid</a><ul><li><a href="#coordinator-nodes">Coordinator Nodes</a></li><li><a href="#overlord">Overlord？？</a></li><li><a href="#broker">Broker</a></li><li><a href="#router">Router？？</a></li><li><a href="#historical">Historical</a></li><li><a href="#middlemanager">MiddleManager？？？</a></li><li><a href="#real-time-node">Real-time Node</a></li></ul></li><li><a href="#辅助">辅助</a><ul><li><a href="#zookeeper">Zookeeper</a></li><li><a href="#metadata-storage">Metadata Storage</a></li><li><a href="#deep-storage">Deep Storage</a></li></ul></li></ul></li><li><a href="#数据">数据</a><ul><li><a href="#column">Column</a></li><li><a href="#segment">Segment</a><ul><li><a href="#segment的存储格式">Segment的存储格式</a><ul><li><a href="#descriptorjson">descriptor.json</a></li><li><a href="#indexzip">index.zip</a><ul><li><a href="#versionbin">version.bin</a></li><li><a href="#metasmooth">meta.smooth</a></li><li><a href="#xxxxxsmoosh文件">xxxxx.smoosh文件</a></li><li><a href="#指标列的存储格式">指标列的存储格式</a></li><li><a href="#维度列的存储格式">维度列的存储格式</a></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><!-- tocstop --><h1><span id="what-is-druid">What is Druid</span></h1><hr><p>Druid是一个开源的实时OLAP系统，可以对超大规模数据提供亚秒级查询，其具有以下特点：    </p><ol><li>列式存储</li><li>倒排索引 （<strong>基于Bitmap实现</strong>）</li><li>分布式的Shared-Nothing架构 （高可用，易扩展是Druid的设计目标）</li><li>实时摄入 （数据被Druid实时摄入后便可以立即查询）<h1><span id="druid架构">Druid架构</span></h1></li></ol><hr><ol><li>实时摄入的过程： 实时数据会首先按行摄入Real-time Nodes，Real-time Nodes会①先将每行的数据加入到1个map中，等达到一定的行数或者大小限制时，②Real-time Nodes 就会将内存中的map 持久化到磁盘中，③Real-time Nodes 会按照segmentGranularity将一定时间段内的小文件merge为一个大文件，生成Segment，④然后将Segment上传到Deep Storage（HDFS，S3）中，⑤Coordinator知道有Segment生成后，会通知相应的Historical Node下载对应的Segment，并负责该Segment的查询。</li><li>离线摄入的过程： 离线摄入的过程比较简单，就是直接通过MR job 生成Segment，剩下的逻辑和实时摄入相同：</li><li>用户查询过程： 用户的查询都是直接发送到Broker Node，Broker Node会将查询分发到Real-time节点和Historical节点，然后将结果合并后返回给用户。</li></ol><p><img src="/images/pasted-7.png" alt="upload successful"></p><h2><span id="节点分类">节点分类</span></h2><p><strong>Master</strong>:  <strong>Coordinator</strong> and <strong>Overlord</strong> processes, manages data availability and ingestion.<br><strong>Query</strong>: Runs <strong>Broker</strong> and optional <strong>Router</strong> processes, handles queries from external clients.<br><strong>Data</strong>: Runs <strong>Historical</strong> and <strong>MiddleManager</strong> processes, executes ingestion workloads and stores all queryable data.</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><h3><span id="druid">druid</span></h3><h4><span id="coordinator-nodes">Coordinator Nodes</span></h4><p>Coordinator 节点主要负责Segment的管理。Coordinator 节点会通知Historical节点加载新Segment，删除旧Segment，复制Segment，以及Segment间的复杂均衡。</p><p>Coordinator 节点依赖ZK确定Historical的存活和集群Segment的分布。</p><h4><span id="overlord">Overlord？？</span></h4><h4><span id="broker">Broker</span></h4><p>Broker 节点是Druid查询的入口，主要负责查询的分发和Merge。 之外，Broker还会对不可变的Segment的查询结果进行LRU缓存。</p><h4><span id="router">Router？？</span></h4><h4><span id="historical">Historical</span></h4><p>Historical 节点是整个Druid集群的骨干，主要负责加载不可变的segment，并负责Segment的查询（注意，<strong>Segment必须加载到Historical 的内存中才可以提供查询</strong>）。Historical 节点是无状态的，所以可以轻易的横向扩展和快速恢复。Historical 节点load和un-load segment是依赖ZK的，但是即使ZK挂掉，Historical依然可以对已经加载的Segment提供查询，只是不能再load 新segment，drop旧segment。</p><h4><span id="middlemanager">MiddleManager？？？</span></h4><h4><span id="real-time-node">Real-time Node</span></h4><p>实时节点主要负责数据的实时摄入，实时数据的查询，将实时数据转为Segment，将Segment Hand off 给Historical 节点。</p><h3><span id="辅助">辅助</span></h3><h4><span id="zookeeper">Zookeeper</span></h4><p>Druid依赖ZK实现服务发现，数据拓扑的感知，以及Coordinator的选主。</p><h4><span id="metadata-storage">Metadata Storage</span></h4><p>Metadata storage（Mysql） 主要用来存储 Segment和配置的元数据。当有新Segment生成时，就会将Segment的元信息写入metadata store, Coordinator 节点会监控Metadata store 从而知道何时load新Segment，何时drop旧Segment。注意，查询时不会涉及Metadata store。</p><h4><span id="deep-storage">Deep Storage</span></h4><p>Deep storage (S3 and HDFS)是作为Segment的永久备份，查询时同样不会涉及Deep storage。</p><h2><span id="数据">数据</span></h2><h3><span id="column">Column</span></h3><p><img src="/images/pasted-9.png" alt="upload successful"><br>Druid中的列主要分为3类：时间列，维度列，指标列。Druid在数据摄入和查询时都依赖时间列，这也是合理的，因为多维分析一般都带有时间维度。    </p><blockquote><p>维度和指标是OLAP系统中常见的概念，维度主要是事件的属性，在查询时一般用来filtering 和 group by，指标是用来聚合和计算的，一般是数值类型，像count,sum，min，max等。</p></blockquote><p>Druid中的维度列支持String，Long，Float，不过只有String类型支持倒排索引；指标列支持Long，Float，Complex， 其中Complex指标包含HyperUnique，Cardinality，Histogram，Sketch等复杂指标。强类型的好处是可以更好的对每1列进行编码和压缩， 也可以保证数据索引的高效性和查询性能。</p><h3><span id="segment">Segment</span></h3><p>前面提到过，Druid中会按时间段生成不可变的带倒排索引的列式文件，这个文件就称之为Segment，Segment是Druid中数据存储、复制、均衡、以及计算的基本单元， <strong>Segment由dataSource_beginTime_endTime_version_shardNumber唯一标识，1个segment一般包含5–10 million行记录，大小一般在300~700mb。</strong>    </p><h4><span id="segment的存储格式">Segment的存储格式</span></h4><p><img src="/images/pasted-12.png" alt="upload successful"><br>上图中给出了Segments在HDFS上的物理存储路径。我们可以看到Segment的路径格式为<code>/druid/segments/&lt;dataSource&gt;/&lt;intervalStart&gt;_&lt;intervalEnd&gt;/&lt;segmentGenerateTime&gt;/&lt;shardNum&gt;/</code><br>该目录下包括两个文件：</p><ol><li>descriptor.json</li><li>index.zip</li></ol><p>其中：</p><h5><span id="descriptorjson">descriptor.json</span></h5><p>是这个Segment的描述文件，其内容也保存在Druid集群的元数据的druid_segments表中。其内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;dataSource&quot;: &quot;AD_active_user&quot;,</span><br><span class="line">&quot;interval&quot;: &quot;2018-04-01T00:00:00.000+08:00/2018-04-02T00:00:00.000+08:00&quot;,</span><br><span class="line">&quot;version&quot;: &quot;2018-04-01T00:04:07.022+08:00&quot;,</span><br><span class="line">&quot;loadSpec&quot;: &#123;</span><br><span class="line">&quot;type&quot;: &quot;hdfs&quot;,</span><br><span class="line">&quot;path&quot;: &quot;/druid/segments/AD_active_user/20180401T000000.000+0800_20180402T000000.000+0800/2018-04-01T00_04_07.022+08_00/1/index.zip&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;dimensions&quot;: &quot;appkey,spreadid,pkgid&quot;,</span><br><span class="line">&quot;metrics&quot;: &quot;myMetrics,count,offsetHyperLogLog&quot;,</span><br><span class="line">&quot;shardSpec&quot;: &#123;</span><br><span class="line">&quot;type&quot;: &quot;numbered&quot;,</span><br><span class="line">&quot;partitionNum&quot;: 1,</span><br><span class="line">&quot;partitions&quot;: 0</span><br><span class="line">&#125;,</span><br><span class="line">&quot;binaryVersion&quot;: 9,</span><br><span class="line">&quot;size&quot;: 168627,</span><br><span class="line">&quot;identifier&quot;: &quot;AD_active_user_2018-04-01T00:00:00.000+08:00_2018-04-02T00:00:00.000+08:00_2018-04-01T00:04:07.022+08:00_1&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h5><span id="indexzip">index.zip</span></h5><p>文件中包括了该segment中的数据。它包含了以下三类文件：</p><pre><code>* version.bin文件* meta.smooth文件* 数据文件(xxxxx.smoosh文件)</code></pre><h6><span id="versionbin">version.bin</span></h6><p>Segment内部标识的版本号，标识为v9. 内容为00000009</p><h6><span id="metasmooth">meta.smooth</span></h6><p>文件主要包含每1列的文件名和文件的偏移量。<br><img src="/images/pasted-13.png" alt="upload successful"></p><h6><span id="xxxxxsmoosh文件">xxxxx.smoosh文件</span></h6><p><img src="/images/pasted-10.png" alt="upload successful"><br>(D1、D2表示列)</p><blockquote><p>（注，druid为了减少文件描述符，将1个segment的所有列都合并到1个大的smoosh中，由于druid访问segment文件的时候采用MMap的方式，所以单个smoosh文件的大小不能超过2G，如果超过2G，就会写到下一个smoosh文件）。</p></blockquote><p><strong>在smoosh文件中，数据是按列存储中，包含时间列，维度列和指标列，其中每1列会包含2部分：ColumnDescriptor和binary数据</strong>。其中ColumnDescriptor主要保存每1列的数据类型和Serde的方式。<br>    smoosh文件中还有index.drd文件和metadata.drd文件，<strong>①</strong>其中index.drd主要包含该segment有哪些列，哪些维度，该Segment的时间范围以及使用哪种bitmap；<strong>②</strong>metadata.drd主要包含是否需要聚合，指标的聚合函数，查询粒度，时间戳字段的配置等。<br> xxxxx从0开始编号，最大为2G，是为了满足Java内存文件映射MapperByteBuffer限制.<br> 在segment生成阶段，Druid会为每列数据生成对应的索引，并将压缩后的聚合数据以及索引存入到以列命名的drd文件中(比如上segment的__time.drd,index.drd,appkey.drd…)。所有的<em>drd文件最终被合并成</em>.smoosh文件。其中压缩编码后的每列原始数据包含两部分<br>  使用Jackson序列化的ColumnDescriptor，包含列数据的元信息，比如数据的类型，是否包含multi-value。。。<br>其余部分为编码以后的二进制数据。</p><h6><span id="指标列的存储格式">指标列的存储格式</span></h6><p>指标列在xxxxx.smoosh文件中。<br>指标列的存储格式如上图所示：<br><img src="/images/pasted-14.png" alt="upload successful"></p><ol><li>version</li><li>value个数</li><li>每个block的value的个数（druid对Long和Float类型会按block进行压缩，block的大小是64K）</li><li>压缩类型 （druid目前主要有LZ4和LZF俩种压缩算法）</li><li>编码类型 （druid对Long类型支持差分编码和Table编码两种方式，Table编码就是将long值映射到int，当指标列的基数小于256时，druid会选择Table编码，否则会选择差分编码）</li><li>编码的header （以差分编码为例，header中会记录版本号，base value，每个value用几个bit表示）</li><li>每个block的header （主要记录版本号，是否允许反向查找，value的数量，列名长度和列名）</li><li>每1列具体的值</li></ol><p><img src="/images/pasted-15.png" alt="upload successful"></p><h6><span id="维度列的存储格式">维度列的存储格式</span></h6><p><img src="/images/pasted-16.png" alt="upload successful"><br>维度列在xxxxx.smoosh文件中。<br>String维度的存储格式如上图所示，前面提到过，时间列，维度列，指标列由两部分组成：ColumnDescriptor和binary数据。 String维度的binary数据主要由3部分组成：dict，字典编码后的id数组，用于倒排索引的bitmap。</p><p>以上图中的D2维度列为例，总共有4行，前3行的值是meituan，第4行的值是dianing。Druid中dict的实现十分简单，就是一个hashmap。图中dict的内容就是将meituan编码为0，dianping编码为1。 Id数组的内容就是用编码后的ID替换掉原始值，所以就是[1,1,1,0]。第3部分的倒排索引就是用bitmap表示某个值是否出现在某行中，如果出现了，bitmap对应的位置就会置为1，如图：meituan在前3行中都有出现，所以倒排索引1：[1,1,1,0]就表示meituan在前3行中出现。</p><p>显然，倒排索引的大小是列的基数*总的行数，如果没有处理的话结果必然会很大。不过好在如果维度列如果基数很高的话，bitmap就会比较稀疏，而稀疏的bitmap可以进行高效的压缩。</p><p>参考：<br><a href="https://blog.bcmeng.com/post/druid-storage.html#string-%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F" target="_blank" rel="noopener">https://blog.bcmeng.com/post/druid-storage.html#string-%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F</a><br><a href="http://druid.io/docs/0.14.0-incubating/design/index.html" target="_blank" rel="noopener">http://druid.io/docs/0.14.0-incubating/design/index.html</a></p>]]></content>
      
      
      <categories>
          
          <category> druid </category>
          
      </categories>
      
      
        <tags>
            
            <tag> druid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>druid流式数据摄入</title>
      <link href="/2019/04/24/druid%E6%95%B0%E6%8D%AE%E6%91%84%E5%85%A5/"/>
      <url>/2019/04/24/druid%E6%95%B0%E6%8D%AE%E6%91%84%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#实时数据摄入">实时数据摄入</a><ul><li><a href="#pull">pull</a></li><li><a href="#push">push</a></li></ul></li></ul><!-- tocstop --><p>对于流式数据的摄入，Druid提供了两种方式，分别是push和pull</p><ol><li>pull:需要启动一个实时节点，通过不同的firehose摄入不同的流式数据，firehose可以被认为Druid接入不同数据源的适配器。</li><li>push:需要启动Tranquility或是Kafka索引服务。通过HTTP调用的方式进行数据摄入   </li></ol><p>离线数据摄入:可以通过Realtime节点摄入，也可以通过索引节点启动任务摄入</p><h1><span id="实时数据摄入">实时数据摄入</span></h1><p><a href="http://druid.io/docs/latest/ingestion/stream-ingestion.html" target="_blank" rel="noopener">官网Loading Streams</a></p><h2><span id="pull">pull</span></h2><p>由于Realtime Node 没有提供高可用，可伸缩等特性<br><del>可以认为已废弃</del><br>高版本中提供新的pull方式即<strong>Kafka Indexing Service (Stream Pull)</strong><br><a href="http://druid.io/docs/latest/tutorials/tutorial-kafka.html" target="_blank" rel="noopener">官网Kafka-indexing-service_1</a><br><a href="http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html" target="_blank" rel="noopener">官网Kafka-indexing-service_2</a></p><h2><span id="push">push</span></h2><p>Tranquility 是一个Scala库，它通过索引服务实现数据实时的摄入。它之所以存在，是因为Indexing service API属于低层面的。Tranquility是对索引服务进行抽象封装， <strong>对使用者屏蔽了 创建任务，处理分区、复制、服务发现和shema rollover等环节</strong>。<br>通过Tranquility 的数据摄入，可以分为两种方式</p><pre><code>1. Tranquility Server：发送方可以通过Tranquility Server 提供的HTTP接口，向Druid发送数据。   [官网demo](http://druid.io/docs/latest/tutorials/tutorial-tranquility.html)2. Tranquility Kafka：发送发可以先将数据发送到Kafka,Tranquility Kafka会根据配置从Kafka获取数据，并写到Druid中。  [官网demo](http://druid.io/docs/latest/ingestion/stream-push.html)</code></pre><p>参考：<br><a href="http://druid.io/docs/latest/tutorials/tutorial-kafka.html" target="_blank" rel="noopener">http://druid.io/docs/latest/tutorials/tutorial-kafka.html</a><br><a href="http://druid.io/docs/latest/ingestion/stream-ingestion.html" target="_blank" rel="noopener">http://druid.io/docs/latest/ingestion/stream-ingestion.html</a></p>]]></content>
      
      
      <categories>
          
          <category> druid </category>
          
      </categories>
      
      
        <tags>
            
            <tag> druid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>druid为什么要调时区</title>
      <link href="/2019/04/24/druid%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%B0%83%E6%97%B6%E5%8C%BA/"/>
      <url>/2019/04/24/druid%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%B0%83%E6%97%B6%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<h1><span id="utc时间ampgmt时间">UTC时间&amp;GMT时间</span></h1><hr><p>UTC时间是时间标准时间（Universal Time Coordinated），UTC是根据原子钟来计算时间，误差非常小。</p><p>UTC也是指零时区的时间，如果要表示其他时区的时间，这里要注意没有UTC+0800或者UTC+8这样的表示方式（至少Java里面没有，一般用于口头表示），只有Asia/Shanghai这样的表示方式，详细的时区列表参考这个文档时区列表﻿，不要问我为什么没有北京时区。。。</p><p>GMT时间是根据地球的自转和公转来计算时间，老的时间计量标准，这里我们不过多讨论</p><h1><span id="表达时间方式">表达时间方式</span></h1><hr><p>我们一般表示时间都会带格式以方便理解，例如时间表达式是’2018-09-12 08:00:00’，因为我们在东八区，所以默认是：北京时间2018年9月12号8点整。但是如果是一个美国人看到这个时间，就会认为是美国东部or西部时间的2018年9月12号8点整。所以从这种表达方式很不准确，因为没有指明到底是哪个时区的时间！！！！</p><p>所以准确的表达时间必须带有时区，例如2018-09-12 08:00:00+0800，表达了Asia/Shanghai这个时区的时间2018年9月12号8点整。这里要注意+0800并不是表示加8小时的意思，只是表示这个时间’2018-09-12 08:00:00’是东八区Asia/Shanghai的时间，仅此而已。</p><h1><span id="utc时间的时间戳">UTC时间的时间戳</span></h1><hr><p>讲清楚了时间表达方式，再讲时间戳。其实时间戳是没有时区概念的，或者说时间戳只能是0时区的。时间戳是从1970-01-11 00:00:00+0000开始的（原因大家都知道），也就是在’1970-01-11 00:00:00+0000’这个时间点，时间戳是0。再换句话说在’1970-01-11 08:00:00+0800’时间戳也是0。这也是Java里时间组件的默认方式，不管用户输入的人类可识别的时间是什么格式，在内部统一存的是时间戳。</p><p>例如时间是’2018-09-01 <strong>08</strong>:00:00+0800’，那么使用date.getTime()获取到时间戳是1535760000000；时间是’2018-09-01 <strong>00</strong>:00:00+0000’，获取到时间戳也是1535760000000。而两者的日期格式差了8小时，而只是时区不同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ssZ&quot;);</span><br><span class="line">System.out.println(sdf.parse(&quot;2018-09-01 08:00:00+0800&quot;).getTime());</span><br><span class="line">System.out.println(sdf.parse(&quot;2018-09-01 00:00:00+0000&quot;).getTime());</span><br></pre></td></tr></table></figure></p><p><strong>时间戳只能是0时区的，所以8时区的时间如果不指定时区也会按照0时区来转化时间戳。此时将时间戳转化为日期格式自然就少了8小时</strong></p><p>可以观察到这2行代码的输出都是1535760000000，这就证明了上面的观点。再啰嗦2点：</p><ol><li>第一行代码DateFormat中Z表示时区，所以String类型格式时间带上+0800这种表达式，就能正确获取时间戳了。</li><li>SimpleDateFormat是线程不安全的，不要用</li></ol><h1><span id="时区设置">时区设置</span></h1><hr><p>为什么我们写以下代码的时候，程序能正确知道我们的时区呢？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">System.out.println(sdf2.parse(&quot;2018-09-01 08:00:00&quot;).getTime());</span><br></pre></td></tr></table></figure></p><p>因为我们在mac上设置了时区.<br>在Java中也可以设置时区</p><ol><li>启动设置<br><code>java -Duser.timezone=Asia/Shanghai -jar xxx.jar</code></li><li><p>代码中设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TimeZone.setDefault(TimeZone.getTimeZone(&quot;UTC&quot;));</span><br><span class="line">SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">System.out.println(sdf.parse(&quot;2018-09-01 08:00:00&quot;).getTime());</span><br></pre></td></tr></table></figure></li><li><p>单次处理生效，建议使用joda的时间包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;joda-time&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;joda-time&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.9.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">DateTimeFormatter dateTimeFormatter = DateTimeFormat.forPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;).withChronology(ISOChronology.getInstance(DateTimeZone.forID(&quot;Asia/Shanghai&quot;)));</span><br><span class="line">System.out.println(dateTimeFormatter.parseDateTime(&quot;2018-09-01 08:00:00&quot;).getMillis());</span><br></pre></td></tr></table></figure></li></ol><p>参考：<br><a href="https://www.cnblogs.com/oldtrafford/p/9680791.html" target="_blank" rel="noopener">https://www.cnblogs.com/oldtrafford/p/9680791.html</a></p>]]></content>
      
      
      <categories>
          
          <category> druid </category>
          
      </categories>
      
      
        <tags>
            
            <tag> druid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka问题集锦</title>
      <link href="/2019/04/24/kafka%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/"/>
      <url>/2019/04/24/kafka%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#kafka为什么要在topic里加入分区的概念">kafka为什么要在topic里加入分区的概念？</a></li><li><a href="#为什么有了partition还需要segment">为什么有了partition还需要segment</a></li></ul><!-- tocstop --><h1><span id="kafka为什么要在topic里加入分区的概念">kafka为什么要在topic里加入分区的概念？</span></h1><hr><p>topic是逻辑的概念，partition是物理的概念，对用户来说是透明的。</p><p>producer只需要关心消息发往哪个topic，而consumer只关心自己订阅哪个topic，并不关心每条消息存于整个集群的哪个broker。  为了性能考虑，如果topic内的消息只存于一个broker，那这个broker会成为瓶颈，无法做到水平扩展。</p><p>所以把topic内的数据分布到整个集群就是一个自然而然的设计方式。</p><p>Partition的引入就是解决水平扩展问题的一个方案。  </p><p>每个partition可以被认为是一个无限长度的数组，新数据顺序追加进这个数组。</p><p>物理上，每个partition对应于一个文件夹。一个broker上可以存放多个partition。</p><p>这样，producer可以将数据发送给多个broker上的多个partition，consumer也可以并行从多个broker上的不同paritition上读数据，实现了水平扩展.     </p><h1><span id="为什么有了partition还需要segment">为什么有了partition还需要segment</span></h1><hr><p>consume消费数据并不需要等到segment写满，只要有一条数据被commit，就可以立马被消费  </p><p>segment对应一个文件（实现上对应2个文件，一个数据文件，一个索引文件），一个partition对应一个文件夹，一个partition里理论上可以包含任意多个segment。</p><p>所以partition可以认为是在segment上做了一层包装。   </p><p>如果不引入segment，一个partition直接对应一个文件（应该说两个文件，一个数据文件，一个索引文件），那这个文件会一直增大。</p><p>同时，在做data purge时，需要把文件的前面部分给删除，不符合kafka对文件的顺序写优化设计方案。</p><p>引入segment后，每次做data purge，只需要把旧的segment整个文件删除即可，保证了每个segment的顺序写.</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka的副本机制</title>
      <link href="/2019/04/24/kafka%E7%9A%84%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/04/24/kafka%E7%9A%84%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#分区和副本">分区和副本</a></li><li><a href="#副本分配算法">副本分配算法</a></li><li><a href="#kafka中的副本">kafka中的副本</a></li><li><a href="#isr集合中的副本必须满足的条件">ISR集合中的副本必须满足的条件</a></li><li><a href="#副本同步时的两个重要概念">副本同步时的两个重要概念</a></li><li><a href="#副本协同机制">副本协同机制</a><ul><li><a href="#第一次发送fetch请求">第一次发送fetch请求</a></li><li><a href="#第二次发送fetch请求">第二次发送fetch请求</a></li></ul></li></ul><!-- tocstop --><h1><span id="分区和副本">分区和副本</span></h1><p>partition的副本被称为replica，每个分区可以有多个副本，并且在副本集中会存在一个leader副本，所有的读写请求都会通过leader完成，follower复制<strong>只负责备份数据</strong>。</p><p>副本会均匀分配到多台broker上，当leader节点挂掉之后，会从副本集中重新选出一个副本作为leader继续提供服务</p><h1><span id="副本分配算法">副本分配算法</span></h1><ol><li>将所有N Broker和待分配的i个Partition排序.</li><li>将第i个Partition分配到第(i mod n)个Broker上.</li><li>将第i个Partition的第j个副本分配到第((i + j) mod n)个Broker上.</li></ol><h1><span id="kafka中的副本">kafka中的副本</span></h1><hr><ol><li>leader副本：响应客户端的读写请求</li><li>follow副本：备份leader的数据，<strong>不进行读写操作</strong></li><li>ISR副本：leader副本和所有能够与leader副本保持基本同步的follow副本，如 果follow副本和leader副本数据同步速度过慢，该follow将会被T出ISR副本</li></ol><p>Leader副本处理分区的所有读写请求并维护自身以及Follower副本的状态。当Leader副本失效时，会从Follower副本中选举一个新的Leader副本对外提供读写服务。</p><h1><span id="isr集合中的副本必须满足的条件">ISR集合中的副本必须满足的条件</span></h1><hr><ol><li>副本<strong>所在的节点</strong>与zk相连</li><li>副本的最后一条消息和leader副本的最后一条消息的差值不能超过阈值<code>replica.lag.time.max.ms</code>默认为10秒。如果该follower在此时间间隔之内没有追上leader,则该follower将会被踢出ISR   </li></ol><p>满足上面2个条件则认为该副本或者节点处于同步中（in sync）。Leader副本会追踪所有同步中的节点，一旦一个节点宕机或者落后太久，Leader就会将该节点从同步副本中ISR列表中移除。Follower从Leader副本同步数据，不是同步，也不是单纯的异步。而是采用了一个同步列表的方式来做了同步和异步的折中。</p><h1><span id="副本同步时的两个重要概念">副本同步时的两个重要概念</span></h1><hr><ol><li>LEO（Last end offset） <strong>记录了该副本底层日志中的下一条消息的offset</strong>，例如LEO为10，那么当前的offset为9</li><li>HW （High water）<strong>标记着可消费的消息</strong>，ISR列表中最小的LEO作为一个分区的HW。HW之前表示已经提交的消息，消费者只能消费已经提交的，HW之后的消息消费者不能消费，因为HW之后的消息表示还没有被ISR列表中的Follower同步；<br>对于同一个副本而言HW不会大于LEO，小于等于HW的消息将会被认为是已备份的。</li></ol><p><img src="/images/pasted-6.png" alt="upload successful"><br>也就是说消息写入到所有副本的日志中才算提交，才可以被消费者消费。<strong>这是对消费者来说的，生产者是否要等待消息都被写入所有副本之后才收到返回是另外一回事</strong>，这个可以通过acks来配置，kafka为生产者提供3种消息确认机制（request.required.acks参数）：</p><ol><li>acks=0，生产者无需等待代理返回确认，就是可以连续发送，但是无法保证消息是否被代理收到。</li><li>acks=1，生产者需要等待Leader副本以及成功写入日志。这种方式降级了消息丢失的可能性，但是也只是Leader写入日志而不管Follower是否写入。</li><li>acks=-1，Leader副本和所有Follower都写入日志才会向生产者发送确认信息。</li></ol><h1><span id="副本协同机制">副本协同机制</span></h1><hr><p><strong>producer将消息发送到该partition的leader上，leader会把消息写入其本地log，每个follower都从leader pull数据。在follower收到消息并且将消息写入本地log之后会向leader发送ack，一旦leader收到了ISR中所有replica的ACK，该消息就被认为已经commit了，leader会增加HW并向producer发送ACK</strong>  </p><h2><span id="第一次发送fetch请求">第一次发送fetch请求</span></h2><hr><p>follower发送fetch请求，并带上自己的LEO<br>leader端的操作 </p><ol><li>当producer发送一个消息给leader之后，leader会把消息写入磁盘 </li><li>然后leader会更新LEO，这时候尝试更新HW，<strong>HW是取LEO和remoteLEO的较小值</strong>，这时候HW依然为0.（<strong>remoteLEO取所有Follower发过来的offset中最小的一个</strong>） </li><li>把消息内容和当前的HW值发送给follower副本</li></ol><p>当收到响应之后，follower端的操作 </p><ol><li>将消息写入本地log，同时更新LEO </li><li>比较本地LEO和返回的HW,比较他们取小值赋值给HW</li></ol><h2><span id="第二次发送fetch请求">第二次发送fetch请求</span></h2><hr><p>leader </p><ol><li>更新remoteLEO=1 </li><li>更新HW为1 </li><li>把数据和当前HW返回给follower，如果这时候没有数据，则返回空</li></ol><p>follower收到response之后 </p><ol><li>如果有数据则写入本地日志，并且更新LEO </li><li>更新HW的值<br>到目前为止，consumer就可以消费offset=0的消息了</li></ol><p>原文：<a href="https://blog.csdn.net/qq_39907763/article/details/82697452" target="_blank" rel="noopener">https://blog.csdn.net/qq_39907763/article/details/82697452</a></p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka是靠什么机制保持高可靠，高可用的？</title>
      <link href="/2019/04/24/Kafka%E6%98%AF%E9%9D%A0%E4%BB%80%E4%B9%88%E6%9C%BA%E5%88%B6%E4%BF%9D%E6%8C%81%E9%AB%98%E5%8F%AF%E9%9D%A0%EF%BC%8C%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84%EF%BC%9F/"/>
      <url>/2019/04/24/Kafka%E6%98%AF%E9%9D%A0%E4%BB%80%E4%B9%88%E6%9C%BA%E5%88%B6%E4%BF%9D%E6%8C%81%E9%AB%98%E5%8F%AF%E9%9D%A0%EF%BC%8C%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>这个问题等同于“说说 Acks 参数对消息持久化的影响？”，只是后者把问题描述的更明确</p><!-- toc --><ul><li><a href="#如何保证宕机的时候数据不丢失问题">如何保证宕机的时候数据不丢失？(问题)</a></li><li><a href="#多副本冗余的高可用机制">多副本冗余的高可用机制</a></li><li><a href="#多副本之间数据如何同步">多副本之间数据如何同步？</a></li><li><a href="#isr-到底指的是什么东西">ISR 到底指的是什么东西？</a></li><li><a href="#acks-参数的含义">Acks 参数的含义</a><ul><li><a href="#第一种选择是把-acks-参数设置为-0">第一种选择是把 Acks 参数设置为 0</a></li><li><a href="#第二种选择是设置-acks-1">第二种选择是设置 Acks = 1</a></li><li><a href="#最后一种情况就是设置-acksall">最后一种情况，就是设置 Acks=all</a></li></ul></li><li><a href="#最后的思考">最后的思考</a></li></ul><!-- tocstop --><h1><span id="如何保证宕机的时候数据不丢失问题">如何保证宕机的时候数据不丢失？(问题)</span></h1><hr><p>如果想理解这个 Acks 参数的含义，首先就得搞明白 Kafka 的高可用架构原理。</p><p>比如下面的图里就是表明了对于每一个 Topic，<strong>我们都可以设置它包含几个 Partition，每个 Partition 负责存储这个 Topic 一部分的数据</strong>。</p><p>然后 Kafka 的 Broker 集群中，每台机器上都存储了一些 Partition，也就存放了 Topic 的<strong>一部分数据</strong>，这样就实现了 Topic 的数据分布式存储在一个 Broker 集群上。</p><p><img src="/images/pasted-0.png" alt="upload successful"><br>但是有一个问题，万一一个 Kafka Broker 宕机了，此时上面存储的数据不就丢失了吗？</p><p>没错，这就是一个比较大的问题了，分布式系统的数据丢失问题，是它首先必须要解决的，一旦说任何一台机器宕机，此时就会导致数据的丢失。</p><h1><span id="多副本冗余的高可用机制">多副本冗余的高可用机制</span></h1><hr><p>所以如果大家去分析任何一个分布式系统的原理，比如说 Zookeeper、Kafka、Redis Cluster、Elasticsearch、HDFS，等等。</p><p>其实它们都有自己内部的一套多副本冗余的机制，多副本冗余几乎是现在任何一个优秀的分布式系统都一般要具备的功能。</p><p>在 Kafka 集群中，每个 Partition 都有多个副本，其中一个副本叫做 Leader，其他的副本叫做 Follower，如下图：</p><p><img src="/images/pasted-2.png" alt="upload successful"><br>如上图所示，假设一个 Topic 拆分为了 3 个 Partition，分别是 Partition0，Partiton1，Partition2，此时每个 Partition 都有 2 个副本。</p><p>比如 Partition0 有一个副本是 Leader，另外一个副本是 Follower，Leader 和 Follower 两个副本是分布在不同机器上的。</p><p>这样的多副本冗余机制，可以保证任何一台机器挂掉，都不会导致数据彻底丢失，因为起码还是有副本在别的机器上的。</p><h1><span id="多副本之间数据如何同步">多副本之间数据如何同步？</span></h1><hr><p>接着我们就来看看多个副本之间数据是如何同步的？其实任何一个 Partition，只有 Leader 是对外提供读写服务的。</p><p>也就是说，如果有一个客户端往一个 Partition 写入数据，此时一般就是写入这个 Partition 的 Leader 副本。</p><p>然后 Leader 副本接收到数据之后，Follower 副本会不停的给它发送请求尝试去拉取最新的数据，拉取到自己本地后，写入磁盘中。</p><p>如下图所示：</p><p><img src="/images/pasted-1.png" alt="upload successful"></p><h1><span id="isr-到底指的是什么东西">ISR 到底指的是什么东西？</span></h1><hr><p>既然大家已经知道了 Partiton 的多副本同步数据的机制了，那么就可以来看看 ISR 是什么了。</p><p>ISR 全称是“In-Sync Replicas”，也就是保持同步的副本，它的含义就是，跟 Leader 始终保持同步的 Follower 有哪些。</p><p>大家可以想一下 ，如果说某个 Follower 所在的 Broker 因为 JVM FullGC 之类的问题，导致自己卡顿了，无法及时从 Leader 拉取同步数据，那么是不是会导致 Follower 的数据比 Leader 要落后很多？</p><p>所以这个时候，就意味着 Follower 已经跟 Leader 不再处于同步的关系了。</p><p>但是只要 Follower 一直及时从 Leader 同步数据，就可以保证它们是处于同步的关系的。</p><p>所以每个 Partition 都有一个 ISR，这个 ISR 里一定会有 Leader 自己，因为 Leader 肯定数据是最新的，然后就是那些跟 Leader 保持同步的 Follower，也会在 ISR 里。</p><h1><span id="acks-参数的含义">Acks 参数的含义</span></h1><p>铺垫了那么多的东西，最后终于可以进入主题来聊一下 Acks 参数的含义了。</p><p>如果大家没看明白前面的那些副本机制、同步机制、ISR 机制，那么就无法充分的理解 Acks 参数的含义，这个参数实际上决定了很多重要的东西。</p><p><strong>首先这个 Acks 参数，是在 Kafka Producer，也就是生产者客户端里设置的</strong>。</p><p>也就是说，你往 Kafka 写数据的时候，就可以来设置这个 Acks 参数。然后这个参数实际上有三种常见的值可以设置，分别是：0、1 和 all。</p><h2><span id="第一种选择是把-acks-参数设置为-0">第一种选择是把 Acks 参数设置为 0</span></h2><p>意思就是我的 Kafka Producer 在客户端，只要把消息发送出去，不管那条数据有没有在哪怕 Partition Leader 上落到磁盘，我就不管它了，直接就认为这个消息发送成功了。</p><p>如果你采用这种设置的话，那么你必须注意的一点是，可能你发送出去的消息还在半路。</p><p>结果呢，Partition Leader 所在 Broker 就直接挂了，然后结果你的客户端还认为消息发送成功了，此时就会导致这条消息就丢失了。</p><p><img src="/images/pasted-3.png" alt="upload successful"></p><h2><span id="第二种选择是设置-acks-1">第二种选择是设置 Acks = 1</span></h2><p>意思就是说只要 Partition Leader 接收到消息而且写入本地磁盘了，就认为成功了，不管它其他的 Follower 有没有同步过去这条消息了。</p><p>这种设置其实是 Kafka 默认的设置，大家请注意，<strong>划重点！这是默认的设置</strong>。</p><p>也就是说，默认情况下，你要是不管 Acks 这个参数，只要 Partition Leader 写成功就算成功。</p><p>但是这里有一个问题，万一 Partition Leader 刚刚接收到消息，Follower 还没来得及同步过去，结果 Leader 所在的 Broker 宕机了，此时也会导致这条消息丢失，因为人家客户端已经认为发送成功了。</p><p><img src="/images/pasted-4.png" alt="upload successful"></p><h2><span id="最后一种情况就是设置-acksall">最后一种情况，就是设置 Acks=all</span></h2><p>这个意思就是说，Partition Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些 Follower 都要把消息同步过去，才能认为这条消息是写入成功了。</p><p>如果说 Partition Leader 刚接收到了消息，但是结果 Follower 没有收到消息，此时 Leader 宕机了，那么客户端会感知到这个消息没发送成功，他会重试再次发送消息过去。</p><p>此时可能 Partition2 的 Follower 变成 Leader 了，此时 ISR 列表里只有最新的这个 Follower 转变成的 Leader 了，那么只要这个新的 Leader 接收消息就算成功了。</p><p><img src="/images/pasted-5.png" alt="upload successful"></p><h1><span id="最后的思考">最后的思考</span></h1><p>Acks=all 就可以代表数据一定不会丢失了吗？当然不是，如果你的 Partition 只有一个副本，也就是一个 Leader，任何 Follower 都没有，你认为 acks=all 有用吗？</p><p>当然没用了，因为 ISR 里就一个 Leader，它接收完消息后宕机，也会导致数据丢失。</p><p>所以说，这个 Acks=all，必须跟 ISR 列表里<strong>至少有 2 个以上的副本配合使用，起码是有一个 Leader 和一个 Follower 才可以</strong>。</p><p>这样才能保证说写一条数据过去，一定是 2 个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。</p><p>所以希望大家把这篇文章好好理解一下，对大家出去面试，或者工作中用 Kafka 都是很好的一个帮助。</p><p>参考：<br><a href="https://mp.weixin.qq.com/s/9vIy6R8IMRU2KoIMzhHHhw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/9vIy6R8IMRU2KoIMzhHHhw</a></p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>404.html</title>
      <link href="/404.html"/>
      <url>/404.html</url>
      
        <content type="html"><![CDATA[<!DOCTYPE HTML><html><head><meta name="generator" content="Hexo 3.8.0">  <meta http-equiv="content-type" content="text/html;charset=utf-8;">  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">  <meta name="robots" content="all">  <meta name="robots" content="index,follow">  <link rel="stylesheet" type="text/css" href="https://qzone.qq.com/gy/404/style/404style.css"></head><body>  <script type="text/plain" src="http://www.qq.com/404/search_children.js" charset="utf-8" homepageurl="/" homepagename="回到我的主页">  </script>  <script src="https://qzone.qq.com/gy/404/data.js" charset="utf-8"></script>  <script src="https://qzone.qq.com/gy/404/page.js" charset="utf-8"></script></body></html>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
